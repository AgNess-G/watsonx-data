{"metadata": {"kernelspec": {"name": "python311", "display_name": "Python 3.11 with Spark", "language": "python3"}, "language_info": {"name": "python", "version": "3.11.9", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat_minor": 4, "nbformat": 4, "cells": [{"cell_type": "markdown", "source": "# Step 0\n\n## - Click on the menu to the right\n\n## - \"Insert Project Token\"\n\n## - Use the downward arrow icon on the top menu to move the cell down and begin running the notebook", "metadata": {}}, {"cell_type": "code", "source": "", "metadata": {"id": "cfa55044-e7e0-4e80-b65d-27433b7dfad2", "msg_id": "8adacfd8-b400-4a22-ae30-fb99fbd3aafe"}, "outputs": [], "execution_count": 5}, {"cell_type": "markdown", "source": "# Load Data into Milvus for RAG\n\n\n# 1. Set up the environment\n\n## Install libraries\n\nwe need to install the pymilvus package to the watsonx.ai Python environment. ", "metadata": {}}, {"cell_type": "code", "source": "!pip install grpcio==1.60.0 \n!pip install pymilvus", "metadata": {"id": "18d833d6-d74f-4692-a764-4fcf2ceefe17", "msg_id": "356f5dda-dade-4816-85e3-1787e214e952"}, "outputs": [{"name": "stdout", "text": "Collecting grpcio==1.60.0\n  Downloading grpcio-1.60.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\nDownloading grpcio-1.60.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: grpcio\n  Attempting uninstall: grpcio\n    Found existing installation: grpcio 1.54.3\n    Uninstalling grpcio-1.54.3:\n      Successfully uninstalled grpcio-1.54.3\nSuccessfully installed grpcio-1.60.0\nCollecting pymilvus\n  Downloading pymilvus-2.5.0-py3-none-any.whl.metadata (5.7 kB)\nRequirement already satisfied: setuptools>69 in ./python/lib/python/site-packages (from pymilvus) (72.1.0)\nRequirement already satisfied: grpcio<=1.67.1,>=1.49.1 in ./python/lib/python/site-packages (from pymilvus) (1.60.0)\nRequirement already satisfied: protobuf>=3.20.0 in ./python/lib/python/site-packages (from pymilvus) (4.21.12)\nCollecting python-dotenv<2.0.0,>=1.0.1 (from pymilvus)\n  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\nCollecting ujson>=2.0.0 (from pymilvus)\n  Downloading ujson-5.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.3 kB)\nRequirement already satisfied: pandas>=1.2.4 in ./python/lib/python/site-packages (from pymilvus) (2.1.4)\nCollecting milvus-lite>=2.4.0 (from pymilvus)\n  Downloading milvus_lite-2.4.10-py3-none-manylinux2014_x86_64.whl.metadata (9.0 kB)\nRequirement already satisfied: tqdm in ./python/lib/python/site-packages (from milvus-lite>=2.4.0->pymilvus) (4.66.4)\nRequirement already satisfied: numpy<2,>=1.23.2 in ./python/lib/python/site-packages (from pandas>=1.2.4->pymilvus) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in ./python/lib/python/site-packages (from pandas>=1.2.4->pymilvus) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in ./python/lib/python/site-packages (from pandas>=1.2.4->pymilvus) (2024.1)\nRequirement already satisfied: tzdata>=2022.1 in ./python/lib/python/site-packages (from pandas>=1.2.4->pymilvus) (2023.3)\nRequirement already satisfied: six>=1.5 in ./python/lib/python/site-packages (from python-dateutil>=2.8.2->pandas>=1.2.4->pymilvus) (1.16.0)\nDownloading pymilvus-2.5.0-py3-none-any.whl (212 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m212.8/212.8 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading milvus_lite-2.4.10-py3-none-manylinux2014_x86_64.whl (49.4 MB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m49.4/49.4 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\nDownloading ujson-5.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: ujson, python-dotenv, milvus-lite, pymilvus\nSuccessfully installed milvus-lite-2.4.10 pymilvus-2.5.0 python-dotenv-1.0.1 ujson-5.10.0\n", "output_type": "stream"}], "execution_count": 4}, {"cell_type": "markdown", "source": "# !!RESTART THE KERNAL AFTER pymilvus install!! \n\nCertain dependencies need to be persisted. Restarting the kernal allows this to occur", "metadata": {}}, {"cell_type": "code", "source": "!pip install ipython-sql==0.4.1\n!pip install sqlalchemy==1.4.46\n!pip install sqlalchemy==1.4.46 \"pyhive[presto]\"\n!pip install python-dotenv\n!pip install sentence_transformers\n!pip install langchain-community\n\n# clean up the libraries not required", "metadata": {"id": "0a8b9b47-22fe-4a70-8d1a-78b635a986a6", "msg_id": "4c9fabd7-47b9-4538-91ca-6cf71cf28ddf"}, "outputs": [{"name": "stdout", "text": "Collecting ipython-sql==0.4.1\n  Downloading ipython_sql-0.4.1-py3-none-any.whl.metadata (17 kB)\nCollecting prettytable<1 (from ipython-sql==0.4.1)\n  Downloading prettytable-0.7.2.zip (28 kB)\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: ipython>=1.0 in ./python/lib/python/site-packages (from ipython-sql==0.4.1) (8.20.0)\nRequirement already satisfied: sqlalchemy>=0.6.7 in ./python/lib/python/site-packages (from ipython-sql==0.4.1) (2.0.25)\nRequirement already satisfied: sqlparse in ./python/lib/python/site-packages (from ipython-sql==0.4.1) (0.5.2)\nRequirement already satisfied: six in ./python/lib/python/site-packages (from ipython-sql==0.4.1) (1.16.0)\nRequirement already satisfied: ipython-genutils>=0.1.0 in ./python/lib/python/site-packages (from ipython-sql==0.4.1) (0.2.0)\nRequirement already satisfied: decorator in ./python/lib/python/site-packages (from ipython>=1.0->ipython-sql==0.4.1) (5.1.1)\nRequirement already satisfied: jedi>=0.16 in ./python/lib/python/site-packages (from ipython>=1.0->ipython-sql==0.4.1) (0.19.1)\nRequirement already satisfied: matplotlib-inline in ./python/lib/python/site-packages (from ipython>=1.0->ipython-sql==0.4.1) (0.1.6)\nRequirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in ./python/lib/python/site-packages (from ipython>=1.0->ipython-sql==0.4.1) (3.0.43)\nRequirement already satisfied: pygments>=2.4.0 in ./python/lib/python/site-packages (from ipython>=1.0->ipython-sql==0.4.1) (2.15.1)\nRequirement already satisfied: stack-data in ./python/lib/python/site-packages (from ipython>=1.0->ipython-sql==0.4.1) (0.2.0)\nRequirement already satisfied: traitlets>=5 in ./python/lib/python/site-packages (from ipython>=1.0->ipython-sql==0.4.1) (5.7.1)\nRequirement already satisfied: pexpect>4.3 in ./python/lib/python/site-packages (from ipython>=1.0->ipython-sql==0.4.1) (4.8.0)\nRequirement already satisfied: typing-extensions>=4.6.0 in ./python/lib/python/site-packages (from sqlalchemy>=0.6.7->ipython-sql==0.4.1) (4.11.0)\nRequirement already satisfied: greenlet!=0.4.17 in ./python/lib/python/site-packages (from sqlalchemy>=0.6.7->ipython-sql==0.4.1) (3.0.1)\nRequirement already satisfied: parso<0.9.0,>=0.8.3 in ./python/lib/python/site-packages (from jedi>=0.16->ipython>=1.0->ipython-sql==0.4.1) (0.8.3)\nRequirement already satisfied: ptyprocess>=0.5 in ./python/lib/python/site-packages (from pexpect>4.3->ipython>=1.0->ipython-sql==0.4.1) (0.7.0)\nRequirement already satisfied: wcwidth in ./python/lib/python/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=1.0->ipython-sql==0.4.1) (0.2.5)\nRequirement already satisfied: executing in ./python/lib/python/site-packages (from stack-data->ipython>=1.0->ipython-sql==0.4.1) (0.8.3)\nRequirement already satisfied: asttokens in ./python/lib/python/site-packages (from stack-data->ipython>=1.0->ipython-sql==0.4.1) (2.0.5)\nRequirement already satisfied: pure-eval in ./python/lib/python/site-packages (from stack-data->ipython>=1.0->ipython-sql==0.4.1) (0.2.2)\nDownloading ipython_sql-0.4.1-py3-none-any.whl (21 kB)\nBuilding wheels for collected packages: prettytable\n  Building wheel for prettytable (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for prettytable: filename=prettytable-0.7.2-py3-none-any.whl size=13695 sha256=e5b18e639fc22ef00a49ba74df2c03b0c30894686101ae30b90c763909c13763\n  Stored in directory: /home/spark/shared/.cache/pip/wheels/b1/e4/65/051f9bd54c89d377db5ca942a672439c36cbf5ceff8e8902e0\nSuccessfully built prettytable\nInstalling collected packages: prettytable, ipython-sql\nSuccessfully installed ipython-sql-0.4.1 prettytable-0.7.2\nCollecting sqlalchemy==1.4.46\n  Downloading SQLAlchemy-1.4.46-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\nRequirement already satisfied: greenlet!=0.4.17 in ./python/lib/python/site-packages (from sqlalchemy==1.4.46) (3.0.1)\nDownloading SQLAlchemy-1.4.46-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: sqlalchemy\n  Attempting uninstall: sqlalchemy\n    Found existing installation: SQLAlchemy 2.0.25\n    Uninstalling SQLAlchemy-2.0.25:\n      Successfully uninstalled SQLAlchemy-2.0.25\nSuccessfully installed sqlalchemy-1.4.46\nRequirement already satisfied: sqlalchemy==1.4.46 in ./python/lib/python/site-packages (1.4.46)\nCollecting pyhive[presto]\n  Downloading PyHive-0.7.0.tar.gz (46 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m46.5/46.5 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: greenlet!=0.4.17 in ./python/lib/python/site-packages (from sqlalchemy==1.4.46) (3.0.1)\nRequirement already satisfied: future in ./python/lib/python/site-packages (from pyhive[presto]) (0.18.3)\nRequirement already satisfied: python-dateutil in ./python/lib/python/site-packages (from pyhive[presto]) (2.8.2)\nRequirement already satisfied: requests>=1.0.0 in ./python/lib/python/site-packages (from pyhive[presto]) (2.32.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in ./python/lib/python/site-packages (from requests>=1.0.0->pyhive[presto]) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in ./python/lib/python/site-packages (from requests>=1.0.0->pyhive[presto]) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in ./python/lib/python/site-packages (from requests>=1.0.0->pyhive[presto]) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in ./python/lib/python/site-packages (from requests>=1.0.0->pyhive[presto]) (2024.7.4)\nRequirement already satisfied: six>=1.5 in ./python/lib/python/site-packages (from python-dateutil->pyhive[presto]) (1.16.0)\nBuilding wheels for collected packages: pyhive\n  Building wheel for pyhive (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for pyhive: filename=PyHive-0.7.0-py3-none-any.whl size=53872 sha256=c7ceb9f38c4086691a88a977f3ec62e1b7d41b6ecf89a8580e26b06d30b173b6\n  Stored in directory: /home/spark/shared/.cache/pip/wheels/11/32/63/d1d379f01c15d6488b22ed89d257b613494e4595ed9b9c7f1c\nSuccessfully built pyhive\nInstalling collected packages: pyhive\nSuccessfully installed pyhive-0.7.0\nRequirement already satisfied: python-dotenv in ./python/lib/python/site-packages (1.0.1)\nCollecting sentence_transformers\n  Downloading sentence_transformers-3.3.1-py3-none-any.whl.metadata (10 kB)\nCollecting transformers<5.0.0,>=4.41.0 (from sentence_transformers)\n  Downloading transformers-4.47.0-py3-none-any.whl.metadata (43 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m43.5/43.5 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: tqdm in ./python/lib/python/site-packages (from sentence_transformers) (4.66.4)\nRequirement already satisfied: torch>=1.11.0 in ./python/lib/python/site-packages (from sentence_transformers) (2.1.2)\nRequirement already satisfied: scikit-learn in ./python/lib/python/site-packages (from sentence_transformers) (1.3.0)\nRequirement already satisfied: scipy in ./python/lib/python/site-packages (from sentence_transformers) (1.11.4)\nRequirement already satisfied: huggingface-hub>=0.20.0 in ./python/lib/python/site-packages (from sentence_transformers) (0.20.3)\nRequirement already satisfied: Pillow in ./python/lib/python/site-packages (from sentence_transformers) (10.3.0)\nRequirement already satisfied: filelock in ./python/lib/python/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in ./python/lib/python/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2023.10.0)\nRequirement already satisfied: requests in ./python/lib/python/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.2)\nRequirement already satisfied: pyyaml>=5.1 in ./python/lib/python/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in ./python/lib/python/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (4.11.0)\nRequirement already satisfied: packaging>=20.9 in ./python/lib/python/site-packages (from huggingface-hub>=0.20.0->sentence_transformers) (23.2)\nRequirement already satisfied: sympy in ./python/lib/python/site-packages (from torch>=1.11.0->sentence_transformers) (1.12)\nRequirement already satisfied: networkx in ./python/lib/python/site-packages (from torch>=1.11.0->sentence_transformers) (2.8.4)\nRequirement already satisfied: jinja2 in ./python/lib/python/site-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\nCollecting huggingface-hub>=0.20.0 (from sentence_transformers)\n  Downloading huggingface_hub-0.26.5-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy>=1.17 in ./python/lib/python/site-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (1.26.4)\nCollecting regex!=2019.12.17 (from transformers<5.0.0,>=4.41.0->sentence_transformers)\n  Downloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m40.5/40.5 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting tokenizers<0.22,>=0.21 (from transformers<5.0.0,>=4.41.0->sentence_transformers)\n  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nCollecting safetensors>=0.4.1 (from transformers<5.0.0,>=4.41.0->sentence_transformers)\n  Downloading safetensors-0.4.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\nRequirement already satisfied: joblib>=1.1.1 in ./python/lib/python/site-packages (from scikit-learn->sentence_transformers) (1.2.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in ./python/lib/python/site-packages (from scikit-learn->sentence_transformers) (2.2.0)\nRequirement already satisfied: MarkupSafe>=2.0 in ./python/lib/python/site-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in ./python/lib/python/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in ./python/lib/python/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.7)\nRequirement already satisfied: urllib3<3,>=1.21.1 in ./python/lib/python/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in ./python/lib/python/site-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2024.7.4)\nRequirement already satisfied: mpmath>=0.19 in ./python/lib/python/site-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\nDownloading sentence_transformers-3.3.1-py3-none-any.whl (268 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading transformers-4.47.0-py3-none-any.whl (10.1 MB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m59.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n\u001b[?25hDownloading huggingface_hub-0.26.5-py3-none-any.whl (447 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m447.8/447.8 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading regex-2024.11.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (792 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m792.7/792.7 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading safetensors-0.4.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (435 kB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m435.0/435.0 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: safetensors, regex, huggingface-hub, tokenizers, transformers, sentence_transformers\n  Attempting uninstall: huggingface-hub\n    Found existing installation: huggingface-hub 0.20.3\n    Uninstalling huggingface-hub-0.20.3:\n      Successfully uninstalled huggingface-hub-0.20.3\n  Attempting uninstall: tokenizers\n    Found existing installation: tokenizers 0.15.2\n    Uninstalling tokenizers-0.15.2:\n      Successfully uninstalled tokenizers-0.15.2\nSuccessfully installed huggingface-hub-0.26.5 regex-2024.11.6 safetensors-0.4.5 sentence_transformers-3.3.1 tokenizers-0.21.0 transformers-4.47.0\n", "output_type": "stream"}], "execution_count": 1}, {"cell_type": "markdown", "source": "# Step 1: Document Ingestion\n\n## Load the pdf version of the watsonx.data documentation\n\nLoad the pdf version as an asset in the project using Spark", "metadata": {}}, {"cell_type": "code", "source": "import requests\nfrom pyspark.sql import SparkSession\nimport os\n\nspark = SparkSession.builder \\\n    .appName(\"Download watsonx.data PDF documentation\") \\\n    .getOrCreate()\n\ndef download_pdf(url, local_path):\n    \"\"\"Download the PDF from a URL and save it locally\"\"\"\n    response = requests.get(url)\n    if response.status_code == 200:\n        with open(local_path, 'wb') as file:\n            file.write(response.content)\n        print(f\"PDF downloaded successfully to {local_path}\")\n    else:\n        print(f\"Failed to download PDF. Status code: {response.status_code}\")\n\npdf_url = \"https://www.ibm.com/support/pages/system/files/inline-files/IBM%20watsonx.data%20version%202.0.3.pdf\"  \nlocal_file_path = \"wxd_doc_pdf.pdf\"  \n\ndownload_pdf(pdf_url, local_file_path)\n\nspark.stop()\n", "metadata": {"msg_id": "fc80e4c6-12c3-4e31-9b6e-bef739f4c30f"}, "outputs": [{"name": "stdout", "text": "PDF downloaded successfully to wxd_doc_pdf.pdf\n", "output_type": "stream"}], "execution_count": 10}, {"cell_type": "markdown", "source": "## Code to extract the text from the pdf for embeddings", "metadata": {}}, {"cell_type": "code", "source": "\nfrom langchain_community.document_loaders import DirectoryLoader\nfrom langchain_community.document_loaders import PyPDFLoader\n\nasset_li=wslib.assets.list_assets(\"data_asset\")\nprint(asset_li)\n\nwslib.download_file(\"wxd_doc_pdf\")\n# loaders = [\n#     DirectoryLoader(directory, glob=\"**/*.html\",show_progress=True),\n#     DirectoryLoader(directory, glob=\"**/*.pdf\",show_progress=True, loader_cls=PyPDFLoader),\n#     DirectoryLoader(directory, glob=\"**/*.pptx\",show_progress=True),\n#     DirectoryLoader(directory, glob=\"**/*.docx\",show_progress=True)\n# ]\n\n\n# documents=[]\n# for loader in loaders:\n#     data =loader.load()\n#     documents.extend(data)\nimport fitz # PyMuPDF\n\ndoc = fitz.open(\"wxd_doc_pdf\")\n\npdf_text = \"\"\n\nfor page in doc:\n    pdf_text += page.get_text()\n\n# print(pdf_text)", "metadata": {"id": "a16b9680-5689-4203-82b9-db682c6e8bfb", "msg_id": "9fbae383-5a04-4bd9-a6dc-622128f80307"}, "outputs": [{"name": "stdout", "text": "[{'name': 'wxd_doc_pdf', 'description': None, 'asset_id': '1377b784-7528-4933-ade1-096dafbf2d7a', 'asset_type': 'data_asset', 'tags': None}]\n", "output_type": "stream"}, {"traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)", "Cell \u001b[0;32mIn[13], line 20\u001b[0m\n\u001b[1;32m      7\u001b[0m wslib\u001b[38;5;241m.\u001b[39mdownload_file(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwxd_doc_pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# loaders = [\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#     DirectoryLoader(directory, glob=\"**/*.html\",show_progress=True),\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m#     DirectoryLoader(directory, glob=\"**/*.pdf\",show_progress=True, loader_cls=PyPDFLoader),\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m#     data =loader.load()\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m#     documents.extend(data)\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfitz\u001b[39;00m \u001b[38;5;66;03m# PyMuPDF\u001b[39;00m\n\u001b[1;32m     22\u001b[0m doc \u001b[38;5;241m=\u001b[39m fitz\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwxd_doc_pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m pdf_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n", "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fitz'"], "ename": "ModuleNotFoundError", "evalue": "No module named 'fitz'", "output_type": "error"}], "execution_count": 13}, {"cell_type": "markdown", "source": "# Step 2: Document Chunking\n\nTo manage the large texts, we can divide the data into manageble chunks by logical units such as paragraphs, sentences, or fixed token lengths", "metadata": {}}, {"cell_type": "markdown", "source": "### Option 1: Chunking by Paragraphs", "metadata": {}}, {"cell_type": "code", "source": "def chunk_by_paragraphs(text):\n    paragraphs = text.split(\"\\n\\n\") # Assuming paragraphs are separated by two newlines\n    return [p.strip() for p in paragraphs if p.strip()]\n\nchunks = chunk_by_paragraphs(pdf_text)\n\nprint(chunks[:5]) # Preview of the first 5 chunks", "metadata": {"id": "12022d7e-0129-4493-9099-78b4a25ebce9"}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "### Option 2: Chunking by Sentences\n\nIf the document structure is more fluid and paragraphs are not clearly defined, you could break the text into sentences. You can use _nltk_ or a similar library for sentence tokennization", "metadata": {}}, {"cell_type": "code", "source": "import nltk\n\nnltk.download('punkt')\n\ndef chunk_by_sentence(text):\n    sentences = nltk.sent_tokenize(text)\n    return sentences\n\nchunks = chunk_by_sentence(pdf_text)\n\nprint(chunks[:5]) # Preview of the first 5 chunks", "metadata": {"id": "a5e90117-3793-4727-b22b-2f3814df300b"}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "### Option 3: Chunking by Token length\n\nFor more control over the chunk size, you can spilt the text into chunks of a fixed number of tokens", "metadata": {}}, {"cell_type": "code", "source": "def chunk_by_tokens(text, max_tokens=512):\n    tokens = text.split() # Tokenize the whitespace\n    chunks = [tokens[i:i + max_tokens] for i in range(0, len(tokens), max_tokens)]\n    return [' '.join(chunk) for chunk in chunks]\n\nchunks = chunk_by_tokens(pdf_text)\n\nprint(chunks[:5]) # Preview of the first 5 chunks", "metadata": {"id": "ff585cc7-db9c-4af2-96a8-0628b6eb16d6"}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "# Step 3: Embedding Generation", "metadata": {}}, {"cell_type": "code", "source": "wslib.list_connections", "metadata": {"id": "b9a699c2-e297-4b58-8c67-760f0d5340b3"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "# note if you named your Milvus connection something other than \"Milvus Connection\" Please replace the name below\n\nmilvus_credentials = wslib.get_connection(\"Milvus Connection\")\n\n# replace the milvus connection asset in the project", "metadata": {"id": "4967f991-b017-460e-a2df-916dce49b94f"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "#milvus_credentials", "metadata": {"id": "73ec2be5-caa9-4102-813d-072cd5186489"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "from pymilvus import(\n    Milvus,\n    IndexType,\n    Status,\n    connections,\n    FieldSchema,\n    DataType,\n    Collection,\n    CollectionSchema,\n)\n\n\nurl = milvus_credentials['host']\nport = milvus_credentials['port']\napikey = milvus_credentials['password']\napiuser = 'ibmlhapikey'\n\n\nconnections.connect(alias=\"default\", \n                    host=url, \n                    port=port, \n                    user=apiuser, \n                    password=apikey, \n                    secure=True)", "metadata": {"id": "f0aad8eb-08e6-49ef-b868-44d0f246525f"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "# Create a new collection\ncollection_description = 'wxd docs pdf'\ncollection_name = 'wxd_documentation'", "metadata": {"id": "d2dae158-da1b-433a-bc74-0ab4249cf879"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "# Create collection - define fields + schema\n\nfields = [\n    FieldSchema(name=\"document_id\", dtype=DataType.INT64), # Document Id\n    FieldSchema(name=\"chunk_id\",  dtype=DataType.INT64), # Chunk Id\n    FieldSchema(name=\"embedding\", dtype=DataType.FLOAT_VECTOR, dim=384), # embedding dimension\n]\n\n# Create a schema\nschema = CollectionSchema(fields, collection_description)\n\n# Create a collection\ncollection = Collection(collection_name, schema)\n\n# Create index\nindex_params = {\n        'metric_type':'L2',\n        'index_type':\"IVF_FLAT\",\n        'params':{\"nlist\":2048}\n}\n\ncollection.create_index(field_name=\"vector\", index_params=index_params)\n", "metadata": {"id": "9aa62853-7fb8-43c6-be9e-f6480ec4b265"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "# we can run a check to see the collections in our milvus instance and we see the new collection has been created \n\nfrom pymilvus import utility\nutility.list_collections()", "metadata": {"id": "7af099cb-7bc0-4ae2-90fb-284b7d9ce12e"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "# load data into Milvus\nimport pandas as pd\nfrom sentence_transformers import SentenceTransformer\nfrom pymilvus import Collection, connections\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nfor i in range(len(article_titles)):\n    # Create vector embeddings + data\n    model = SentenceTransformer('sentence-transformers/all-minilm-l12-v2') # 384 dim\n    passage_embeddings = model.encode(article_chunks[i])\n\n    basic_collection = Collection(collection_name) \n    data = [\n        article_chunks[i],\n        article_titles[i],\n        passage_embeddings\n    ]\n               \n    out = basic_collection.insert(data)\n    basic_collection.flush()  # Ensures data persistence\n\n    \n    print(\"Wikipedia Article: \\'\" + article_titles[i][0] + \"\\' has been loaded.\")", "metadata": {"id": "5c531cc4-56ec-4dcd-a61f-9cca82e963cb"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "## check to ensure entities have been loaded into the collection\n\nbasic_collection = Collection(collection_name) \n\nbasic_collection.num_entities", "metadata": {"id": "d630914b-ec2a-47b9-92e0-d766c70a9a34"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "", "metadata": {"id": "62512ec5-d1dc-4744-8eb1-aa1a34d14131"}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "# Step 4: Searching with Milvus", "metadata": {}}, {"cell_type": "code", "source": "from sentence_transformers import SentenceTransformer\nfrom pymilvus import(\n    Milvus,\n    IndexType,\n    Status,\n    connections,\n    FieldSchema,\n    DataType,\n    Collection,\n    CollectionSchema,\n)\n\nurl = milvus_credentials['host']\nport = milvus_credentials['port']\napikey = milvus_credentials['password']\napiuser = 'ibmlhapikey'\n\n\nconnections.connect(alias=\"default\", \n                    host=url, \n                    port=port, \n                    user=apiuser, \n                    password=apikey, \n                    secure=True)\n\n\n# Load collection\n\nbasic_collection = Collection(collection_name)      \nbasic_collection.load()\n\n# Query function\ndef query_milvus(query, num_results):\n    \n    # Vectorize query\n    model = SentenceTransformer('sentence-transformers/all-minilm-l12-v2') # 384 dim\n    query_embeddings = model.encode([query])\n\n    # Search\n    search_params = {\n        \"metric_type\": \"L2\", \n        \"params\": {\"nprobe\": 5}\n    }\n    results = basic_collection.search(\n        data=query_embeddings, \n        anns_field=\"vector\", \n        param=search_params,\n        limit=num_results,\n        expr=None, \n        output_fields=['article_text'],\n    )\n    return results", "metadata": {"id": "a91053c4-296e-4f62-b4c6-d171b0b1eaaf"}, "outputs": [], "execution_count": null}, {"cell_type": "markdown", "source": "# Prompt with LLM", "metadata": {"id": "d152a3f3-b1e8-4ee0-a004-18b4170bbe8e"}}, {"cell_type": "code", "source": "## Consider some questions to ask regarding the topic you have chosen \n\n#question_text = \"OTHER QUESTION TEXT\"\n\nquestion_text = \"How to add a new catalog?\"", "metadata": {"id": "a91e98ee-61df-47ce-a8b0-a2c9061d52ac"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "# Query Milvus \n\nnum_results = 3\nresults = query_milvus(question_text, num_results)\n\nrelevant_chunks = []\nfor i in range(num_results):    \n    #print(f\"id: {results[0].ids[i]}\")\n    #print(f\"distance: {results[0].distances[i]}\")\n    text = results[0][i].entity.get('article_text')\n    relevant_chunks.append(text)\n    \n#print(relevant_chunks)", "metadata": {"id": "33a836e6-1a1d-4f1e-80f5-5808912d146d"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "def make_prompt(context, question_text):\n    return (f\"{context}\\n\\nPlease answer a question using this text. \"\n          + f\"If the question is unanswerable, say \\\"unanswerable\\\".\"\n          + f\"\\n\\nQuestion: {question_text}\")\n\n\n# Build prompt w/ Milvus results\n# Embed retrieved passages(context) and user question into into prompt text\n\ncontext = \"\\n\\n\".join(relevant_chunks)\nprompt = make_prompt(context, question_text)\n\nprint(prompt)", "metadata": {"id": "85a0ccc0-43ef-4976-be3b-87a99a34f9be"}, "outputs": [], "execution_count": null}, {"cell_type": "code", "source": "from ibm_watson_machine_learning.foundation_models import Model\nfrom ibm_watson_machine_learning.metanames import GenTextParamsMetaNames as GenParams\n\n# Model Parameters\nparams = {\n        GenParams.DECODING_METHOD: \"greedy\",\n        GenParams.MIN_NEW_TOKENS: 1,\n        GenParams.MAX_NEW_TOKENS: 500,\n        GenParams.TEMPERATURE: 0,\n}\n\n\n# please note if using a cloud account in a different geography the cloud URL will be different \n# Refer to this list: \n#    Dallas - https://us-south.ml.cloud.ibm.com\n#    London - https://eu-gb.ml.cloud.ibm.com\n#    Frankfurt - https://eu-de.ml.cloud.ibm.com\n#    Tokyo - https://jp-tok.ml.cloud.ibm.com\n\ncreds = {\n    \"url\": 'https://us-south.ml.cloud.ibm.com',\n    \"apikey\": milvus_credentials['password'] \n}\n\nmodel = Model(\n        model_id='ibm/granite-13b-chat-v2', \n        #model_id='meta-llama/llama-2-70b-chat', \n        params=params, credentials=creds, \n        project_id=wslib.here.get_ID()\n)\n\n# Prompt LLM\nresponse = model.generate_text(prompt)\nprint(f\"Question: {question_text}{response}\")", "metadata": {"id": "924b3368-a649-48fb-a90c-81eee6f81ed6"}, "outputs": [], "execution_count": null}]}